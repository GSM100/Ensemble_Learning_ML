{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTl3xUn0jAGOO+wXgDlnLk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["1. Can we use Bagging for regression problems?\n","\n","-  Yes. Bagging can be applied to both classification and regression.\n","For regression, predictions from individual models are averaged instead of taking a majority vote.\n","\n","2. What is the difference between multiple model training and single model training?\n","\n","- Single model training → one model learns patterns (e.g., one Decision Tree).\n","\n","- Multiple model training (ensemble) → several models are trained, and their results are combined → usually more accurate and robust.\n","\n","3. Explain the concept of feature randomness in Random Forest.\n","\n","- Random Forest introduces randomness by:\n","\n","- Bootstrapping rows (sampling with replacement).\n","\n","- At each split, it selects a random subset of features instead of all features.\n","\n","-  This decorrelates trees and improves generalization.\n","\n","4. What is OOB (Out-of-Bag) Score?\n","\n","- In Bagging/Random Forest, ~37% of samples are left out during bootstrapping.\n","\n","- These are called Out-of-Bag samples.\n","\n","- They are used to estimate model performance without needing a separate validation set.\n","\n","5. How can you measure the importance of features in a Random Forest model?\n","\n","- Mean Decrease in Impurity (Gini importance) → measures how much a feature reduces impurity across all trees.\n","\n","- Permutation importance → measures performance drop when the feature is randomly shuffled.\n","\n","6. Explain the working principle of a Bagging Classifier.\n","\n","- Draw bootstrap samples from dataset.\n","\n","- Train a base model (like Decision Tree) on each sample.\n","\n","- Aggregate results: majority vote (classification) / average (regression).\n","\n","7. How do you evaluate a Bagging Classifier’s performance?\n","\n","- Using accuracy, precision, recall, F1-score (for classification).\n","\n","- Using MSE, RMSE, R² (for regression).\n","\n","- Optionally with OOB score.\n","\n","8. How does a Bagging Regressor work?\n","\n","- Same principle as Bagging Classifier, but predictions are averaged instead of - majority vote.\n","\n","9. What is the main advantage of ensemble techniques?\n","\n","- Improved accuracy.\n","\n","- Reduced variance (stability).\n","\n","- Better generalization than a single model.\n","\n","10. What is the main challenge of ensemble methods?\n","\n","- Computational cost (training multiple models).\n","\n","- Complexity (harder to interpret).\n","\n","- Risk of overfitting if not properly tuned.\n","\n","11. Explain the key idea behind ensemble techniques.\n","\n","- “Wisdom of the crowd” → combining multiple weak learners (diverse models) often produces a stronger overall model.\n","\n","12. What is a Random Forest Classifier?\n","\n","- An ensemble of Decision Trees trained with bagging + feature randomness.\n","Final output = majority vote of all trees.\n","\n","13. What are the main types of ensemble techniques?\n","\n","- Bagging (e.g., Random Forest).\n","\n","- Boosting (e.g., AdaBoost, XGBoost, CatBoost, LightGBM).\n","\n","- Stacking (meta-learning).\n","\n","14. What is ensemble learning in machine learning?\n","\n","- The process of combining multiple models (weak/strong learners) to achieve - better predictive performance.\n","\n","15. When should we avoid using ensemble methods?\n","\n","- When interpretability is critical (ensembles are “black-box”).\n","\n","- When computational resources are limited.\n","\n","- When a single strong model already performs well.\n","\n","16. How does Bagging help in reducing overfitting?\n","\n","- By training models on different bootstrap samples, Bagging reduces variance and prevents a single model from memorizing noise.\n","\n","17. Why is Random Forest better than a single Decision Tree?\n","\n","- Decision Trees are prone to overfitting.\n","\n","- Random Forest averages results across many trees → reduces variance, improves stability & accuracy.\n","\n","18. What is the role of bootstrap sampling in Bagging?\n","\n","- Bootstrap sampling ensures diversity among models by providing different subsets of data to each base learner.\n","\n","19. What are some real-world applications of ensemble techniques?\n","\n","- Fraud detection (Boosting).\n","\n","- Medical diagnosis (Random Forest).\n","\n","- Credit scoring (Bagging/Boosting).\n","\n","- Recommendation systems.\n","\n","- Text classification / spam filtering."],"metadata":{"id":"AhPbdA-vm0EL"}},{"cell_type":"markdown","source":["21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy"],"metadata":{"id":"7xTpKx_koWqh"}},{"cell_type":"code","source":["!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TN7LbdStYbl","executionInfo":{"status":"ok","timestamp":1758130008743,"user_tz":-330,"elapsed":5460,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"be7fad99-e023-4124-fde5-559f0b58f1a6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"]}]},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","X, y = load_iris(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Bagging Classifier\n","bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=50, random_state=42)\n","bagging.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = bagging.predict(X_test)\n","print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lYKISCLRobGG","executionInfo":{"status":"ok","timestamp":1758130429309,"user_tz":-330,"elapsed":453,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"7ad7df8b-e4b9-4d88-9261-2addbca2536a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Classifier Accuracy: 1.0\n"]}]},{"cell_type":"markdown","source":["Q22) Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error"],"metadata":{"id":"pSbDNAFJs-e_"}},{"cell_type":"code","source":["from sklearn.datasets import load_diabetes\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import BaggingRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Load dataset\n","X, y = load_diabetes(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Bagging Regressor\n","bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(),\n","                               n_estimators=50, random_state=42)\n","bagging_reg.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred = bagging_reg.predict(X_test)\n","print(\"MSE:\", mean_squared_error(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EN4UDdxTtoln","executionInfo":{"status":"ok","timestamp":1758130451180,"user_tz":-330,"elapsed":451,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"68792a14-4fa4-496b-a136-951b8f3fec7d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE: 2987.0073593984966\n"]}]},{"cell_type":"markdown","source":["Q23.Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."],"metadata":{"id":"QMdaW4zFvs8F"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.ensemble import RandomForestClassifier\n","import pandas as pd\n","\n","# Load dataset\n","data = load_breast_cancer()\n","X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n","\n","# Random Forest Classifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf.fit(X_train, y_train)\n","\n","# Feature Importance\n","importance = pd.DataFrame({'Feature': data.feature_names,\n","                           'Importance': rf.feature_importances_}).sort_values(by='Importance', ascending=False)\n","\n","print(importance.head(10))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mt-Ny4AHvyTl","executionInfo":{"status":"ok","timestamp":1758130562531,"user_tz":-330,"elapsed":1035,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"cb96c243-4232-48dd-c707-714fce49a091"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["                 Feature  Importance\n","7    mean concave points    0.141934\n","27  worst concave points    0.127136\n","23            worst area    0.118217\n","6         mean concavity    0.080557\n","20          worst radius    0.077975\n","22       worst perimeter    0.074292\n","2         mean perimeter    0.060092\n","3              mean area    0.053810\n","26       worst concavity    0.041080\n","0            mean radius    0.032312\n"]}]},{"cell_type":"markdown","source":["Q24) Train a Random Forest Regressor and compare its performance with a single Decision Tree."],"metadata":{"id":"TTr6RB1EwBaL"}},{"cell_type":"code","source":["from sklearn.metrics import r2_score\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","# Random Forest Regressor\n","rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf_reg.fit(X_train, y_train)\n","y_pred_rf = rf_reg.predict(X_test)\n","\n","# Decision Tree Regressor\n","dt_reg = DecisionTreeRegressor(random_state=42)\n","dt_reg.fit(X_train, y_train)\n","y_pred_dt = dt_reg.predict(X_test)\n","\n","print(\"Random Forest R²:\", r2_score(y_test, y_pred_rf))\n","print(\"Decision Tree R²:\", r2_score(y_test, y_pred_dt))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzwU11xwwHXP","executionInfo":{"status":"ok","timestamp":1758130737989,"user_tz":-330,"elapsed":895,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"4304c6ad-b46d-4a8f-a3dd-e630ce91f481"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest R²: 0.8531093915343916\n","Decision Tree R²: 0.7486772486772486\n"]}]},{"cell_type":"markdown","source":["Q25) Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier."],"metadata":{"id":"AfFwe0s-xIEO"}},{"cell_type":"code","source":["rf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n","rf_oob.fit(X_train, y_train)\n","print(\"OOB Score:\", rf_oob.oob_score_)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXqFDO-oxMzV","executionInfo":{"status":"ok","timestamp":1758130929711,"user_tz":-330,"elapsed":922,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"b2ea498f-aada-4370-e21a-b6393c1b30eb"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["OOB Score: 0.9547738693467337\n"]}]},{"cell_type":"markdown","source":["Q26) Train a Bagging Classifier using SVM as a base estimator and print accuracy."],"metadata":{"id":"htVdVSDAxlLe"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_breast_cancer\n","\n","# Load dataset (assuming you want to use the breast cancer dataset from previous cells)\n","data = load_breast_cancer()\n","X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n","\n","\n","# Bagging with SVM\n","bag_svm = BaggingClassifier(estimator=SVC(kernel=\"linear\"),\n","                            n_estimators=10, random_state=42)\n","bag_svm.fit(X_train, y_train)\n","\n","y_pred = bag_svm.predict(X_test)\n","print(\"Bagging Classifier with SVM Accuracy:\", accuracy_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzRhk-40xpAd","executionInfo":{"status":"ok","timestamp":1758131082974,"user_tz":-330,"elapsed":16728,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"e72ce064-5a87-4073-f42f-276711e8d0a3"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Classifier with SVM Accuracy: 0.9590643274853801\n"]}]},{"cell_type":"markdown","source":["Q27) Train a Random Forest Classifier with different numbers of trees and compare accuracy."],"metadata":{"id":"TvxLbQ8zx6Fs"}},{"cell_type":"code","source":["for n in [10, 50, 100, 200]:\n","    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n","    rf.fit(X_train, y_train)\n","    acc = accuracy_score(y_test, rf.predict(X_test))\n","    print(f\"Trees: {n}, Accuracy: {acc:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wOZNcQZyx9jB","executionInfo":{"status":"ok","timestamp":1758131129130,"user_tz":-330,"elapsed":1579,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"3288b619-b13f-4fba-885a-d2bc3a9ade67"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Trees: 10, Accuracy: 0.9649\n","Trees: 50, Accuracy: 0.9708\n","Trees: 100, Accuracy: 0.9708\n","Trees: 200, Accuracy: 0.9708\n"]}]},{"cell_type":"markdown","source":["Q28) Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."],"metadata":{"id":"QmJs316RyXSU"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_breast_cancer\n","\n","bag_log = BaggingClassifier(estimator=LogisticRegression(max_iter=1000),\n","                            n_estimators=20, random_state=42)\n","bag_log.fit(X_train, y_train)\n","\n","y_pred_proba = bag_log.predict_proba(X_test)[:,1]\n","print(\"Bagging + Logistic Regression AUC:\", roc_auc_score(y_test, y_pred_proba))"],"metadata":{"id":"cfzb48lryoua","executionInfo":{"status":"ok","timestamp":1758131434337,"user_tz":-330,"elapsed":455,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}}},"execution_count":22,"outputs":[]}]}